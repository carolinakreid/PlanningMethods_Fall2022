{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: T - Tests and Chi-Square\n",
    "\n",
    "In this notebook, we're going to focus on two statistical tests:\n",
    "\n",
    "> A t-test of means\n",
    "\n",
    "> A Chi-Square test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Reading in our libraries, our dataset, and renaming our variables\n",
    "\n",
    "Just the intro material!  Remember, you need to run all the cells in order - libraries, read data, and rename data, otherwise Python will give you an error message!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Because we're expanding our toolkit to do statistical tests, I need to install a new library \n",
    "#that allows me to do a ttest of means and the chi-square test.  Because reserachpy is not part of the\n",
    "#datahub environment, I actually have to install the library before I can call it in\n",
    "!pip install researchpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, We're going to call in our libraries\n",
    "from IPython.display import Image\n",
    "import researchpy as rp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show our plots in the Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When we start working with nan (missing) values, we can get warnings - we're going to ignore them here\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we're going to read in our data and work with the same extract as we have been\n",
    "\n",
    "chis_df = pd.read_csv('chis_extract_2022_weights.csv')\n",
    "chis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df.rename(columns={\"SRAGE_P1\": \"age\", \"AE_VEGI\":\"ate_veg\",\n",
    "                        \"SRSEX\": \"sex\",\n",
    "                        \"OMBSRR_P1\": \"race_ethnicity\",\n",
    "                        \"POVLL\" : \"pov_cat\",\n",
    "                       \"AK22_P1\" : \"hh_inc\",\n",
    "                       \"AM184\": \"housing_worry\",\n",
    "                       \"CV7_1\":\"covid_lostjob\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df=(chis_df[['age','ate_veg','sex', 'race_ethnicity', 'pov_cat', 'hh_inc', 'housing_worry', 'covid_lostjob']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codebook\n",
    "\n",
    "> AE_VEGI: Number of times respondent eats vegetables per week\n",
    "\n",
    "> SRSEX: Self-reported Sex (1= Male, 2=Female)\n",
    "\n",
    "> OMBSRR_P1: Race/ethnicity\n",
    "(1=Hispanic, 2= White NH, 3=Black NH, 4=AmIndian/Alaska Native NH, 5=Asian NH, 6=Other or two or more)\n",
    "\n",
    "> POVLL: poverty level\n",
    "(1 = 0-99% FPL, 2=100-199% FPL, 3=200-299% FPL, 4=300% FPL and above)\n",
    "\n",
    "> AK22_P1: Household Income\n",
    "\n",
    "> AM184: How Often Worry about Paying Rent/Mortgage\n",
    "(1=Very often, 2=Somewhat Often, 3=From Time to Time, 4=Almost Never)\n",
    "\n",
    "> CV7_1: Lost Job due to COVID (1=Yes, 2=No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cleaning my variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should all look familiar - they are just the code cells from my other notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep people who said they ate less than 10 veggies a day\n",
    "chis_df = chis_df[chis_df['ate_veg'] < 71] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided I want to group together anyone that expresses concern, \n",
    "#so I'm going to assign a 1 to 1,2,3, and a 0 to anyone who never worries\n",
    "chis_df['housing_worry_dv']=chis_df['housing_worry'].map({1:1, 2:1, 3:1, 4:0})\n",
    "pd.crosstab(chis_df['housing_worry_dv'], columns='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Because I am most concerned about households living under the poverty line, \n",
    "#I'm going to create a dummy where 1 = under the poverty line, and 0 is above\n",
    "\n",
    "chis_df['inpoverty_dv']=chis_df['pov_cat'].map({1:1, 2:0, 3:0, 4:0})\n",
    "pd.crosstab(chis_df['inpoverty_dv'], columns='Total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chis_df['lostjob_dv']=chis_df['covid_lostjob'].map({1:1, 2:0})\n",
    "pd.crosstab(chis_df['lostjob_dv'], columns='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.0  Testing Bivariate Relationships\n",
    "\n",
    "### 2.1 Hypothesis\n",
    "\n",
    "Let's start by reminding ourselves why we're doing all this data cleaning!  I am interested in understanding whether someone who lost their job due to COVID is more likely to be concerned about paying their rent, which I can use to argue for an extension of eviction moratoria or greater rent relief.\n",
    "\n",
    "**My hypothesis is that people who lost their job due to COVID are more likely to be concerned about paying their rent.**\n",
    "\n",
    ">  Y Variable: How Often Worry about Paying Rent/Mortgage  (AM184 - renamed housing_worry)\n",
    "    (1=Very often, 2=Somewhat Often, 3=From Time to Time, 4=Almost Never)\n",
    "    \n",
    "    > dummy is housing_worry_dv\n",
    "\n",
    ">  X Variable: lost job due to COVID  (CV7_1  - renamed covid_lostjob)\n",
    "\n",
    "    > dummy is lostjob_dv\n",
    "\n",
    ">  Alternate X Variable: Categorical poverty level (POVLL - renamed pov_cat)\n",
    "\n",
    "    > dummy is inpoverty_dv\n",
    "\n",
    "**I'm also going to look at whether folks in poverty eat fewer vegetables, mostly to demonstrate code and concepts!**\n",
    "\n",
    "        > Y Variable: Eat Vegetables (ae_veg - renamed ate_veg)\n",
    "    \n",
    "        > X Variable: Categorical poverty level (POVLL - renamed pov_cat)\n",
    "        \n",
    "            >dummy is inpoverty_dv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Conducting the tests\n",
    "\n",
    "Now that I've cleaned my data, I can start to explore whether or not there are relationships between my Y and X variables.  I'm going to explore whether there are any observable differences in the average number of veggies a person consumes by my poverty variable first (a ttest of means because \"ate_veg\" is a numeric variable).  Then, I'm going to see if a greater proportion of people who lost their job during COVID are concerned about paying their rent/mortgage (a Chi-Square test because both are dummy variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the first test is trying to understand whether the *average* number of veggies eaten (numeric) varies by poverty status (dummy), my first test is going to be a ttest of means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, I'm going to look at the average number of veggies eaten by my poverty dummy\n",
    "chis_df[\"ate_veg\"].groupby(chis_df[\"inpoverty_dv\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's where I'm going to take advantage of the researchpy library and run the ttest function\n",
    "rp.ttest((chis_df[chis_df['inpoverty_dv']==0].ate_veg), (chis_df[chis_df['inpoverty_dv']==1].ate_veg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I could also use the ttest function from scipy.stats, but I like the output from researchpy\n",
    "\n",
    "ttest_ind(chis_df[chis_df['inpoverty_dv'] == 1].ate_veg, chis_df[chis_df['inpoverty_dv'] == 0].ate_veg, equal_var = False, nan_policy=\"omit\")\n",
    "\n",
    "#The equal variance option allows you to specify whether you think the variances\n",
    "#of the two samples are the same.  Try and see what happens when you assume equal variances.  \n",
    "\n",
    "#Setting equal variances as \"false\" is going to give you a more conservative estimate of statistical significance.  \n",
    "\n",
    "#The nanpolicy tells Python to omit observations where the data are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I want to compare two categorical (dummy) variables, so I'm going to use a Chi-Square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check first and make sure we have at least 5 observations in each cell\n",
    "pd.crosstab(index=chis_df[\"housing_worry_dv\"], columns=chis_df[\"lostjob_dv\"], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=chis_df[\"housing_worry_dv\"], columns=chis_df[\"lostjob_dv\"], margins=True, normalize='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's researchpy again, this time for the chi-square test\n",
    "rp.crosstab(chis_df[\"housing_worry_dv\"], chis_df[\"lostjob_dv\"], prop=\"col\", test=\"chi-square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Conclusion\n",
    "\n",
    "The principles of testing for statistical significance are exactly the same as with the ACS, now, we're just using different tests with different probability distributions.  The most important thing is to focus on the **meaning** of what you're testing; that, with the p-value, will allow you to interpret a much broader range of research results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racial Neighborhood Change (2009-2018)\n",
    "\n",
    "In this notebook, I bring us back to ACS data, and show the power of using Python rather than Excel to work with ACS and the associated margins of error and statistical testing.  I also provide an example of using a crosswalk to reconcile different geographies over time.  The notebook also starts to introduce more sophisticated coding structures - these are still new to me too, but they show how programmers move from the \"write out every piece of code\" approach to getting from A to B to more streamlined and automated codes, which can reduce time and errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean ACS data from 2009\n",
    "\n",
    "Due to the Census website changes, they have not yet posted the 2009 5-year ACS for download.  Instead, I downloaded the data from Social Explorer.  The biggest difference is that Social Explorer provides the standard error, not the margin of error, so I will have to do some manipulation of the variables to aggregate my MOEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2009 = pd.read_csv(\"ACS2009.csv\", dtype={'Geo_FIPS': str})\n",
    "\n",
    "# drop extra columns\n",
    "df_2009.drop(columns=['Geo_GEOID', 'Geo_NAME', 'Geo_QName', 'Geo_STUSAB',\n",
    "       'Geo_SUMLEV', 'Geo_GEOCOMP', 'Geo_FILEID', 'Geo_LOGRECNO', 'Geo_US',\n",
    "       'Geo_REGION', 'Geo_DIVISION', 'Geo_STATECE', 'Geo_STATE', 'Geo_COUNTY',\n",
    "       'Geo_COUSUB', 'Geo_PLACE', 'Geo_PLACESE', 'Geo_TRACT', 'Geo_BLKGRP',\n",
    "       'Geo_CONCIT', 'Geo_AIANHH', 'Geo_AIANHHFP', 'Geo_AIHHTLI', 'Geo_AITSCE',\n",
    "       'Geo_AITS', 'Geo_ANRC', 'Geo_CBSA', 'Geo_CSA', 'Geo_METDIV', 'Geo_MACC',\n",
    "       'Geo_MEMI', 'Geo_NECTA', 'Geo_CNECTA', 'Geo_NECTADIV', 'Geo_UA',\n",
    "       'Geo_UACP', 'Geo_CDCURR', 'Geo_SLDU', 'Geo_SLDL', 'Geo_VTD',\n",
    "       'Geo_ZCTA3', 'Geo_ZCTA5', 'Geo_SUBMCD', 'Geo_SDELM', 'Geo_SDSEC',\n",
    "       'Geo_SDUNI', 'Geo_UR', 'Geo_PCI', 'Geo_TAZ', 'Geo_UGA', 'Geo_PUMA5',\n",
    "       'Geo_PUMA1'], inplace=True)\n",
    "\n",
    "df_2009.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "rename_2009 = {'ACS09_5yr_B03002001': \"total_2009\", \n",
    "    'ACS09_5yr_B03002002': \"nh_total_2009\",\n",
    "    'ACS09_5yr_B03002003': \"nhwhite_2009\", \n",
    "    'ACS09_5yr_B03002004' : 'nhblack_2009',\n",
    "    'ACS09_5yr_B03002005':'nhamindian_2009',\n",
    "    'ACS09_5yr_B03002006':\"nhasian_2009\",\n",
    "    'ACS09_5yr_B03002007':\"nhhpi_2009\",\n",
    "    'ACS09_5yr_B03002008':'nhother_2009',\n",
    "    'ACS09_5yr_B03002009':'nhtwoplus_2009',\n",
    "    'ACS09_5yr_B03002010':'nhtwoplincother_2009', \n",
    "    'ACS09_5yr_B03002011':'nhtwoplexother_2009',\n",
    "    'ACS09_5yr_B03002012':'hispanic_2009',\n",
    "    'ACS09_5yr_B03002013':'hwhite_2009',\n",
    "    'ACS09_5yr_B03002014':'hblack_2009',\n",
    "    'ACS09_5yr_B03002015':'hamindian_2009',\n",
    "    'ACS09_5yr_B03002016':'hasian_2009',\n",
    "    'ACS09_5yr_B03002017': 'hhpi_2009',\n",
    "    'ACS09_5yr_B03002018':'hother_2009',\n",
    "    'ACS09_5yr_B03002019':'htwoplus_2009',\n",
    "    'ACS09_5yr_B03002020':'htwoplincother_2009',\n",
    "    'ACS09_5yr_B03002021':'htwoplexother_2009',\n",
    "    'ACS09_5yr_B03002001s': \"total_2009_se\",\n",
    "    'ACS09_5yr_B03002002s': \"nh_total_2009_se\",\n",
    "    'ACS09_5yr_B03002003s': \"nhwhite_2009_se\",\n",
    "    'ACS09_5yr_B03002004s': 'nhblack_2009_se',\n",
    "    'ACS09_5yr_B03002005s':'nhamindian_2009_se',\n",
    "    'ACS09_5yr_B03002006s':\"nhasian_2009_se\",\n",
    "    'ACS09_5yr_B03002007s':\"nhhpi_2009_se\",\n",
    "    'ACS09_5yr_B03002008s':'nhother_2009_se',\n",
    "    'ACS09_5yr_B03002009s':'nhtwoplus_2009_se',\n",
    "    'ACS09_5yr_B03002010s':'nhtwoplincother_2009_se',\n",
    "    'ACS09_5yr_B03002011s':'nhtwoplexother_2009_se',\n",
    "    'ACS09_5yr_B03002012s':'hispanic_2009_se',\n",
    "    'ACS09_5yr_B03002013s':'hwhite_2009_se',\n",
    "    'ACS09_5yr_B03002014s':'hblack_2009_se',\n",
    "    'ACS09_5yr_B03002015s':'hamindian_2009_se',\n",
    "    'ACS09_5yr_B03002016s':'hasian_2009_se',\n",
    "    'ACS09_5yr_B03002017s': 'hhpi_2009_se',\n",
    "    'ACS09_5yr_B03002018s':'hother_2009_se',\n",
    "    'ACS09_5yr_B03002019s':'htwoplus_2009_se',\n",
    "    'ACS09_5yr_B03002020s':'htwoplincother_2009_se',\n",
    "    'ACS09_5yr_B03002021s':'htwoplexother_2009_se',\n",
    "    'ACS09_5yr_B25003001':'hu_2009',\n",
    "    'ACS09_5yr_B25003002':'owner_2009',\n",
    "    'ACS09_5yr_B25003003': 'renter_2009',\n",
    "    'ACS09_5yr_B25003001s': 'hu_2009_se',\n",
    "    'ACS09_5yr_B25003002s':'owner_2009_se',\n",
    "    'ACS09_5yr_B25003003s': 'renter_2009_se'}\n",
    "\n",
    "df_2009.rename(columns=rename_2009, inplace=True)\n",
    "df_2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create MOEs from the SE provided in the raw data\n",
    "moe_fields = list(rename_2009.values())\n",
    "moe_fields = [x for x in moe_fields if \"se\" in x]\n",
    "for i in moe_fields:\n",
    "    df_2009[i] = df_2009[i] * 1.645\n",
    "\n",
    "df_2009.rename(columns={\"total_2009_se\": \"total_2009_moe\",\n",
    "\"nh_total_2009_se\":\"nh_total_2009_moe\",\n",
    "\"nhwhite_2009_se\":\"nhwhite_2009_moe\",\n",
    "'nhblack_2009_se': 'nhblack_2009_moe',\n",
    "'nhamindian_2009_se':'nhamindian_2009_moe',\n",
    "\"nhasian_2009_se\": \"nhasian_2009_moe\",\n",
    "\"nhhpi_2009_se\": \"nhhpi_2009_moe\",\n",
    "'nhother_2009_se' :'nhother_2009_moe',\n",
    "'nhtwoplus_2009_se': 'nhtwoplus_2009_moe',\n",
    "'nhtwoplincother_2009_se': 'nhtwoplincother_2009_moe',\n",
    "'nhtwoplexother_2009_se': 'nhtwoplexother_2009_moe',\n",
    "'hispanic_2009_se': 'hispanic_2009_moe',\n",
    "'hwhite_2009_se': 'hwhite_2009_moe',\n",
    "'hblack_2009_se': 'hblack_2009_moe',\n",
    "'hamindian_2009_se' :'hamindian_2009_moe',\n",
    "'hasian_2009_se':'hasian_2009_moe',\n",
    "'hhpi_2009_se':'hhpi_2009_moe',\n",
    "'hother_2009_se' :'hother_2009_moe',\n",
    "'htwoplus_2009_se' :'htwoplus_2009_moe',\n",
    "'htwoplincother_2009_se': 'htwoplincother_2009_moe',\n",
    "'htwoplexother_2009_se': 'htwoplexother_2009_moe',\n",
    "'hu_2009_se': 'hu_2009_moe',\n",
    "'owner_2009_se' :'owner_2009_moe',\n",
    "'renter_2009_se': 'renter_2009_moe'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate our race/ethnicity columns and then drop the columns we no longer need\n",
    "\n",
    "df_2009['nhothers_2009']=(df_2009['nhamindian_2009_moe'] + df_2009['nhhpi_2009'] + df_2009['nhother_2009'] + df_2009['nhtwoplus_2009'])\n",
    "\n",
    "# sum moe columns\n",
    "\n",
    "df_2009['nhothers_2009_moe']=(np.sqrt(df_2009['nhamindian_2009_moe']**2 + df_2009['nhhpi_2009_moe']**2\n",
    "                                         + df_2009['nhother_2009_moe']**2 + df_2009['nhtwoplus_2009_moe']**2))\n",
    "\n",
    "#keep the variables of interest\n",
    "\n",
    "acs_2009_df = df_2009[['Geo_FIPS', 'total_2009', 'total_2009_moe', 'nhwhite_2009', 'nhwhite_2009_moe',\n",
    "                      'nhblack_2009', 'nhblack_2009_moe', 'nhasian_2009', 'nhasian_2009_moe',\n",
    "                       'hispanic_2009', 'hispanic_2009_moe', 'nhothers_2009', 'nhothers_2009_moe', \n",
    "                       'hu_2009', 'hu_2009_moe',\n",
    "                      'owner_2009', 'owner_2009_moe', 'renter_2009', 'renter_2009_moe']]\n",
    "acs_2009_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above, I used the \"rename\" dictionary to pass a list of variables to my operation, \n",
    "#but I can also create a specific list of variables to use in future code\n",
    "list_2009={'total_2009', 'total_2009_moe', 'nhwhite_2009', 'nhwhite_2009_moe',\n",
    "                      'nhblack_2009', 'nhblack_2009_moe', 'nhasian_2009', 'nhasian_2009_moe',\n",
    "                       'hispanic_2009', 'hispanic_2009_moe', 'nhothers_2009', 'nhothers_2009_moe', \n",
    "                       'hu_2009', 'hu_2009_moe',\n",
    "                      'owner_2009', 'owner_2009_moe', 'renter_2009', 'renter_2009_moe'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Clean 2018 ACS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.read_csv(\"ACS2018.csv\", dtype={'Geo_FIPS': str})\n",
    "\n",
    "# drop extra columns\n",
    "df_2018.drop(columns=['Geo_GEOID', 'Geo_BTTR', 'Geo_BTBG','Geo_NAME', 'Geo_QName', 'Geo_STUSAB',\n",
    "       'Geo_SUMLEV', 'Geo_GEOCOMP', 'Geo_FILEID', 'Geo_LOGRECNO', 'Geo_US',\n",
    "       'Geo_REGION', 'Geo_DIVISION', 'Geo_STATECE', 'Geo_STATE', 'Geo_COUNTY',\n",
    "       'Geo_COUSUB', 'Geo_PLACE', 'Geo_PLACESE', 'Geo_TRACT', 'Geo_BLKGRP',\n",
    "       'Geo_CONCIT', 'Geo_AIANHH', 'Geo_AIANHHFP', 'Geo_AIHHTLI', 'Geo_AITSCE',\n",
    "       'Geo_AITS', 'Geo_ANRC', 'Geo_CBSA', 'Geo_CSA', 'Geo_METDIV', 'Geo_MACC',\n",
    "       'Geo_MEMI', 'Geo_NECTA', 'Geo_CNECTA', 'Geo_NECTADIV', 'Geo_UA',\n",
    "       'Geo_UACP', 'Geo_CDCURR', 'Geo_SLDU', 'Geo_SLDL', 'Geo_VTD',\n",
    "       'Geo_ZCTA3', 'Geo_ZCTA5', 'Geo_SUBMCD', 'Geo_SDELM', 'Geo_SDSEC',\n",
    "       'Geo_SDUNI', 'Geo_UR', 'Geo_PCI', 'Geo_TAZ', 'Geo_UGA', 'Geo_PUMA5',\n",
    "       'Geo_PUMA1'], inplace=True)\n",
    "\n",
    "df_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "rename_2018 = {'ACS18_5yr_B03002001': \"total_2018\", \n",
    "    'ACS18_5yr_B03002002': \"nh_total_2018\",\n",
    "    'ACS18_5yr_B03002003': \"nhwhite_2018\", \n",
    "    'ACS18_5yr_B03002004' : 'nhblack_2018',\n",
    "    'ACS18_5yr_B03002005':'nhamindian_2018',\n",
    "    'ACS18_5yr_B03002006':\"nhasian_2018\",\n",
    "    'ACS18_5yr_B03002007':\"nhhpi_2018\",\n",
    "    'ACS18_5yr_B03002008':'nhother_2018',\n",
    "    'ACS18_5yr_B03002009':'nhtwoplus_2018',\n",
    "    'ACS18_5yr_B03002010':'nhtwoplusincother_2018', \n",
    "    'ACS18_5yr_B03002011':'nhtwoplusexother_2018',\n",
    "    'ACS18_5yr_B03002012':'hispanic_2018',\n",
    "    'ACS18_5yr_B03002013':'hwhite_2018',\n",
    "    'ACS18_5yr_B03002014':'hblack_2018',\n",
    "    'ACS18_5yr_B03002015':'hamindian_2018',\n",
    "    'ACS18_5yr_B03002016':'hasian_2018',\n",
    "    'ACS18_5yr_B03002017': 'hhpi_2018',\n",
    "    'ACS18_5yr_B03002018':'hother_2018',\n",
    "    'ACS18_5yr_B03002019':'htwoplus_2018',\n",
    "    'ACS18_5yr_B03002020':'htwoplincother_2018',\n",
    "    'ACS18_5yr_B03002021':'htwoplexother_2018',\n",
    "    'ACS18_5yr_B03002001s': \"total_2018_se\",\n",
    "    'ACS18_5yr_B03002002s': \"nh_total_2018_se\",\n",
    "    'ACS18_5yr_B03002003s': \"nhwhite_2018_se\",\n",
    "    'ACS18_5yr_B03002004s': 'nhblack_2018_se',\n",
    "    'ACS18_5yr_B03002005s':'nhamindian_2018_se',\n",
    "    'ACS18_5yr_B03002006s':\"nhasian_2018_se\",\n",
    "    'ACS18_5yr_B03002007s':\"nhhpi_2018_se\",\n",
    "    'ACS18_5yr_B03002008s':'nhother_2018_se',\n",
    "    'ACS18_5yr_B03002009s':'nhtwoplus_2018_se',\n",
    "    'ACS18_5yr_B03002010s':'nhtwoplincother_2018_se',\n",
    "    'ACS18_5yr_B03002011s':'nhtwoplexother_2018_se',\n",
    "    'ACS18_5yr_B03002012s':'hispanic_2018_se',\n",
    "    'ACS18_5yr_B03002013s':'hwhite_2018_se',\n",
    "    'ACS18_5yr_B03002014s':'hblack_2018_se',\n",
    "    'ACS18_5yr_B03002015s':'hamindian_2018_se',\n",
    "    'ACS18_5yr_B03002016s':'hasian_2018_se',\n",
    "    'ACS18_5yr_B03002017s': 'hhpi_2018_se',\n",
    "    'ACS18_5yr_B03002018s':'hother_2018_se',\n",
    "    'ACS18_5yr_B03002019s':'htwoplus_2018_se',\n",
    "    'ACS18_5yr_B03002020s':'htwoplincother_2018_se',\n",
    "    'ACS18_5yr_B03002021s':'htwoplexother_2018_se',\n",
    "    'ACS18_5yr_B25003001':'hu_2018',\n",
    "    'ACS18_5yr_B25003002':'owner_2018',\n",
    "    'ACS18_5yr_B25003003': 'renter_2018',\n",
    "    'ACS18_5yr_B25003001s': 'hu_2018_se',\n",
    "    'ACS18_5yr_B25003002s':'owner_2018_se',\n",
    "    'ACS18_5yr_B25003003s': 'renter_2018_se'}\n",
    "\n",
    "df_2018.rename(columns=rename_2018, inplace=True)\n",
    "df_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create MOEs from the SE provided in the raw data\n",
    "moe_fields = list(rename_2018.values())\n",
    "moe_fields = [x for x in moe_fields if \"se\" in x]\n",
    "for i in moe_fields:\n",
    "    df_2018[i] = df_2018[i] * 1.645\n",
    "    \n",
    "df_2018.rename(columns={\"total_2018_se\": \"total_2018_moe\",\n",
    "\"nh_total_2018_se\":\"nh_total_2018_moe\",\n",
    "\"nhwhite_2018_se\":\"nhwhite_2018_moe\",\n",
    "'nhblack_2018_se': 'nhblack_2018_moe',\n",
    "'nhamindian_2018_se':'nhamindian_2018_moe',\n",
    "\"nhasian_2018_se\": \"nhasian_2018_moe\",\n",
    "\"nhhpi_2018_se\": \"nhhpi_2018_moe\",\n",
    "'nhother_2018_se' :'nhother_2018_moe',\n",
    "'nhtwoplus_2018_se': 'nhtwoplus_2018_moe',\n",
    "'nhtwoplincother_2018_se': 'nhtwoplincother_2018_moe',\n",
    "'nhtwoplexother_2018_se': 'nhtwoplexother_2018_moe',\n",
    "'hispanic_2018_se': 'hispanic_2018_moe',\n",
    "'hwhite_2018_se': 'hwhite_2018_moe',\n",
    "'hblack_2018_se': 'hblack_2018_moe',\n",
    "'hamindian_2018_se' :'hamindian_2018_moe',\n",
    "'hasian_2018_se':'hasian_2018_moe',\n",
    "'hhpi_2018_se':'hhpi_2018_moe',\n",
    "'hother_2018_se' :'hother_2018_moe',\n",
    "'htwoplus_2018_se' :'htwoplus_2018_moe',\n",
    "'htwoplincother_2018_se': 'htwoplincother_2018_moe',\n",
    "'htwoplexother_2018_se': 'htwoplexother_2018_moe',\n",
    "'hu_2018_se': 'hu_2018_moe',\n",
    "'owner_2018_se' :'owner_2018_moe',\n",
    "'renter_2018_se': 'renter_2018_moe'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate our race/ethnicity columns and then drop the columns we no longer need\n",
    "\n",
    "df_2018['nhothers_2018']=(df_2018['nhamindian_2018'] + df_2018['nhhpi_2018'] + df_2018['nhother_2018'] + df_2018['nhtwoplus_2018'])\n",
    "\n",
    "# sum moe columns\n",
    "\n",
    "df_2018['nhothers_2018_moe']=(np.sqrt(df_2018['nhamindian_2018_moe']**2 + df_2018['nhhpi_2018_moe']**2\n",
    "                                         + df_2018['nhother_2018_moe']**2 + df_2018['nhtwoplus_2018_moe']**2))\n",
    "\n",
    "acs_2018_df = df_2018[['Geo_FIPS', 'total_2018', 'total_2018_moe', 'nhwhite_2018', 'nhwhite_2018_moe',\n",
    "                      'nhblack_2018', 'nhblack_2018_moe', 'nhasian_2018', 'nhasian_2018_moe',\n",
    "                       'hispanic_2018', 'hispanic_2018_moe', 'nhothers_2018', 'nhothers_2018_moe', \n",
    "                       'hu_2018', 'hu_2018_moe',\n",
    "                      'owner_2018', 'owner_2018_moe', 'renter_2018', 'renter_2018_moe']].copy()\n",
    "acs_2018_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Crosswalk\n",
    "\n",
    "The 2009 ACS data is on 2000 census tracts, while the 2018 data is on 2010 census tracts.  Because we will soon have 2020 Census geographies, learning how to crosswalk is an important skill!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I downloaded this crosswalk from the Brown Longitudinal Database\n",
    "crosswalk = pd.read_csv(\"crosswalk_2000_2010.csv\", \n",
    "                        dtype={'trtid00': str, 'trtid10': str})\n",
    "crosswalk = crosswalk[['trtid00', 'trtid10', 'weight']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join 2009 data to crosswalk\n",
    "\n",
    "The 2009 5-yr ACS data is on 2000 tracts. We are going to crosswalk these to 2010 tracts so they can be joined to the 2018 5-yr ACS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009 = crosswalk.merge(acs_2009_df, left_on=\"trtid00\", right_on=\"Geo_FIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009[crosswalk_2009['trtid00'] == '06001403500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009[(crosswalk_2009['trtid10'] == '06001425104') | (crosswalk_2009['trtid00'] == '06001401000') | (crosswalk_2009['trtid00'] == '06001425100')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply each of the 2009 variables by the crosswalk weight\n",
    "\n",
    "For example, the 2000 tract `01025958000` is split in 2010. It is no longer the same FIPS code so we need to reallocate ~42% to `01025958001` and ~58% to `01025958002`.\n",
    "\n",
    "| trtid00 | trtid10 | weight |\n",
    "| ------- | ------- | ------ |\n",
    "| 01025958000 |\t01025958001 | 0.416454 |\n",
    "| 01025958000 |\t01025958002 | 0.583546 |\n",
    "\n",
    "*NOTE:* Do not reweigh the margins of error (MOE). We are taking a conservative approach and keeping the MOE as is to avoid overestimating the number of tracts with a statistically significant change in tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reweigh_fields = [x for x in list_2009 if \"moe\" not in x]\n",
    "for i in reweigh_fields:\n",
    "    crosswalk_2009[i] = crosswalk_2009[i] * crosswalk_2009['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009[crosswalk_2009['trtid00'] == '06001403500']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum adjusted variables by 2010 census tract fips code\n",
    "\n",
    "The dataset currently has multiple rows for each tract, need to condense by grouping on the 2010 FIPS codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009.head(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum count columns\n",
    "keep_fields = ['trtid10'] + reweigh_fields\n",
    "crosswalk_2009_count = crosswalk_2009[keep_fields].groupby('trtid10').sum()\n",
    "crosswalk_2009_count.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum moe columns\n",
    "keep_fields = [x for x in list_2009 if \"moe\" in x]\n",
    "keep_fields_dict = {}\n",
    "for k in keep_fields:\n",
    "    keep_fields_dict[k] = lambda x: np.sqrt(np.sum(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk_2009_moe = crosswalk_2009.groupby('trtid10').agg(keep_fields_dict)\n",
    "crosswalk_2009_moe.reset_index(inplace=True)\n",
    "crosswalk_2009_moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join back together\n",
    "crosswalk_2009 = crosswalk_2009_count.merge(crosswalk_2009_moe, on=\"trtid10\")\n",
    "crosswalk_2009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join weighted 2009 tracts to 2018 tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = acs_2018_df.merge(crosswalk_2009, left_on=\"Geo_FIPS\", right_on=\"trtid10\")\n",
    "df_join.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate percents (and associated MoEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    # [numerator, denominator]\n",
    "    ['owner', 'hu'],\n",
    "    ['renter', 'hu'],\n",
    "    ['nhwhite', 'total'],\n",
    "    ['nhblack', 'total'],\n",
    "    ['nhasian', 'total'],\n",
    "    ['nhothers', 'total'],\n",
    "    ['hispanic', 'total']\n",
    "]\n",
    "years = ['2009', '2018']\n",
    "\n",
    "for y in years:\n",
    "    for f in fields:\n",
    "        numer = f[0]\n",
    "        denom = f[1]\n",
    "        df_join[\"p_\"+numer+\"_\"+y] = df_join[numer+\"_\"+y]/df_join[denom+\"_\"+y]\n",
    "        df_join[\"p_\"+numer+\"_\"+y+\"_moe\"]=  np.sqrt(df_join[numer+\"_\"+y+\"_moe\"]**2 - \n",
    "                                                  (df_join[\"p_\"+numer+\"_\"+y]**2 * \n",
    "                                                   df_join[denom+\"_\"+y+\"_moe\"]**2)) / df_join[denom+\"_\"+y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if change between 2009 and 2018 is statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in fields:\n",
    "    f = field[0]\n",
    "    print(f)\n",
    "    \n",
    "    # calculate percent change\n",
    "    df_join['p_'+f+'_change'] = df_join['p_'+f+'_2018'] - df_join['p_'+f+'_2009']\n",
    "    df_join['p_'+f+'_change_moe'] = df_join['p_'+f+'_2018_moe']/df_join['p_'+f+'_2009_moe']\n",
    "    \n",
    "    # calculate z-statistic for percent change\n",
    "    df_join['z_'+f] = (df_join['p_'+f+'_2018'] - \n",
    "                           df_join['p_'+f+'_2009']) / np.sqrt(((df_join['p_'+f+'_2018_moe']/1.645)**2) + \n",
    "                                                               ((df_join['p_'+f+'_2009_moe']/1.645)**2))\n",
    "    \n",
    "    # statistically significant increase\n",
    "    print(\"Statistically significant increase:\", len(df_join[df_join['z_'+f]>1.645]))\n",
    "    df_join['s_incr_'+f] = np.where(df_join['z_'+f]>1.645, 1, 0)\n",
    "    \n",
    "    # statistically significant decrease\n",
    "    print(\"Statistically significant decrease:\", len(df_join[df_join['z_'+f]<-1.645]))\n",
    "    df_join['s_decr_'+f] = np.where(df_join['z_'+f]<-1.645, 1, 0)\n",
    "    \n",
    "    # no statistically significant change\n",
    "    print(\"No statistically significant change:\", len(df_join[(df_join['z_'+f]<=1.645)&(df_join['z_'+f]>=-1.645)]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation \n",
    "\n",
    "Do we see any correlation between changes at the census tract level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join[['s_incr_owner', 's_incr_nhwhite']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join[['s_incr_nhwhite', 's_decr_nhblack']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
